from typing import Union
import numpy as np

from tools import load_iris, split_train_test
import matplotlib.pyplot as plt


def sigmoid(x: float) -> float:
    '''
    Calculate the sigmoid of x
    '''
    if(x<-100):
        return 0
    return 1 / (1 + np.exp(-x))


def d_sigmoid(x: float) -> float:
    '''
    Calculate the derivative of the sigmoid of x.
    '''
    return np.exp(-x)/np.power(1+np.exp(-x),2)


def perceptron(
    x: np.ndarray,
    w: np.ndarray
) -> Union[float, float]:
    '''
    Return the weighted sum of x and w as well as
    the result of applying the sigmoid activation
    to the weighted sum
    '''
    sum=0
    for i in range(len(x)):
        sum+= x[i]*w[i]
    return sum, sigmoid(sum)



def ffnn(
    x: np.ndarray,
    M: int,
    K: int,
    W1: np.ndarray,
    W2: np.ndarray,
) -> Union[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    '''
    Computes the output and hidden layer variables for a
    single hidden layer feed-forward neural network.
    '''
    z0=np.insert(x,0,1)
    aOne= []
    z1=[]
    for i in range(M):

        aOne.append(sum(z0*W1[:,i]))
        z1.append(sigmoid(aOne[i]))
    z1.insert(0,1)
    aTwo=[]
    y=[]
    for i in range(K):
        aTwo.append(sum(z1*W2[:,i]))
        y.append(sigmoid(aTwo[i]))
    return y,z0,z1,aOne,aTwo

def backprop(
    x: np.ndarray,
    target_y: np.ndarray,
    M: int,
    K: int,
    W1: np.ndarray,
    W2: np.ndarray
) -> Union[np.ndarray, np.ndarray, np.ndarray]:
    '''
    Perform the backpropagation on given weights W1 and W2
    for the given input pair x, target_y
    '''

    y_k,z0,z1,aOne,aTwo=ffnn(x,M,K,W1,W2)
    dE1= np.zeros((W1.shape[0],W1.shape[1]))
    dE2= np.zeros((W2.shape[0],W2.shape[1]))
    delta_k = y_k -target_y
    delta_y=[]

    #Play with index
    W=W2.T
    for j in range(len(aOne)):
        value=0
        for i in range(K):
            value+=W[i,j+1]*delta_k[i]
        delta_y.append(d_sigmoid(aOne[j])*value)
    '''
    print("Ci" ,delta_y)

    print(len(delta_y))
    print("a1  ", len(aOne), "  a2 ",len(aTwo))
    print("W1 ",W1.shape[0]," ",W1.shape[1])

    print("W2 ",W2.shape[0]," ",W2.shape[1])
    print("z0 ",len(z0), "  z1 ",len(z1))
    '''
    for i in range(dE1.shape[0]):
        for j in range(dE1.shape[1]):
            dE1[i][j] = delta_y[j]*z0[i]
    #print(np.asarray(dE1))

    for i in range(dE2.shape[0]):
        for j in range(dE2.shape[1]):
            dE2[i][j] = delta_k[j]*z1[i]
    #print(np.asarray(dE2))
    return y_k,dE1,dE2
    #print(result)
    #print(y_k)


def train_nn(
    X_train: np.ndarray,
    t_train: np.ndarray,
    M: int,
    K: int,
    W1: np.ndarray,
    W2: np.ndarray,
    iterations: int,
    eta: float
) -> Union[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    '''
    Train a network by:
    1. forward propagating an input feature through the network
    2. Calculate the error between the prediction the network
    made and the actual target
    3. Backpropagating the error through the network to adjust
    the weights.
    '''
    E_total=[]
    misclassification_rate= []
    results = []
    for j in range(iterations):
        sum=np.zeros(X_train.shape[0])
        errors=0
        dE1_total= np.zeros((W1.shape[0],W1.shape[1]))
        dE2_total= np.zeros((W2.shape[0],W2.shape[1]))
        for i,e in enumerate(X_train):
            target_y = np.zeros(K)
            target_y[t_train[i]] = 1.0
            y,dE1,dE2=backprop(e,target_y,M,K,W1,W2)
            position=np.argmax(y)
            dE1_total+=dE1
            dE2_total+=dE2
            if(j==iterations-1):
                results.append(position)
            if(target_y[position]!= 1):
                errors+=1
            for a in range(len(target_y)):
                sum[i]+=target_y[a]*np.log(y[a])+(1-target_y[a])*np.log(1-y[a])
        E_total.append(-np.mean(sum))
        misclassification_rate.append(errors/X_train.shape[0])
        W1 = W1- eta * dE1_total/ X_train.shape[0]
        W2 = W2- eta * dE2_total/ X_train.shape[0]


    return W1,W2,np.asarray(E_total),misclassification_rate,results



def test_nn(
    X: np.ndarray,
    M: int,
    K: int,
    W1: np.ndarray,
    W2: np.ndarray
) -> np.ndarray:
    '''
    Return the predictions made by a network for all features
    in the test set X.
    '''
    results=[]
    for i in range(X.shape[0]):
        y,z,z1,a1,a2=ffnn(X[i],M,K,W1,W2)
        results.append(np.argmax(y))
    return np.asarray(results)

def tester():
    (train_features, train_targets), (test_features, test_targets) = \
        split_train_test(features, targets)
    K = 3  # number of classes
    M = 6
    D = train_features.shape[1]
    # Initialize two random weight matrices
    W1 = 2 * np.random.rand(D + 1, M) - 1
    W2 = 2 * np.random.rand(M + 1, K) - 1
    W1tr, W2tr, Etotal, misclassification_rate, last_guesses =train_nn(train_features, train_targets, M, K, W1, W2, 500, 0.1)
    accuracy_ofTrainer= accurancy(np.asarray(last_guesses),train_targets)
    y_predicted=test_nn(test_features,M,K,W1tr,W2tr)
    accuracy_ofTester= accurancy(np.asarray(y_predicted),test_targets)
    print("Accuracy of trainer:",accuracy_ofTrainer)
    print("Accuracy of tester:",accuracy_ofTester)
    matrixOne=confusion_matrix(np.asarray(last_guesses),train_targets)
    matrixTwo=confusion_matrix(np.asarray(y_predicted),test_targets)
    print("Confusion matrix of the trainer:")
    print(matrixOne)
    print("Confusion matrix of the tester:")
    print(matrixTwo)
    plot_ETotal(Etotal)
    plot_misclassification_rate(misclassification_rate)

def accurancy(
    y_calucated: np.ndarray,
    y_test: np.ndarray
) -> float:
    correctness=0
    for i in range(y_calucated.shape[0]):
        if(y_calucated[i]== y_test[i]):
            correctness+=1
    return correctness/y_test.shape[0]

def confusion_matrix(
    y_pred: np.ndarray,
    target: np.ndarray
)  -> np.matrix:
    confusion_matrix= [[0,0,0],[0,0,0],[0,0,0]]
    for i in range(target.shape[0]):
        confusion_matrix[target[i]][y_pred[i]]+=1
    return np.asmatrix(confusion_matrix)


def plot_ETotal(
    E_total : np.ndarray
):
    plt.plot(E_total)
    plt.savefig("eTotal.png")
    plt.clf()

def plot_misclassification_rate(
    misclassification_rate : np.ndarray

):
    plt.plot(misclassification_rate)
    plt.savefig("misclassification.png")
    plt.clf()

def independent():
    pass


'''
print("----TEST 1.1-----")
print(sigmoid(0.5) == 0.6224593312018546)
print(d_sigmoid(0.2) == 0.24751657271185995)


print("----TEST 1.2 -----")
a =perceptron(np.array([1.0, 2.3, 1.9]),np.array([0.2,0.3,0.1]))
print(a[0]==1.0799999999999998 , " ", a[1]==0.7464939833376621)
b= perceptron(np.array([0.2,0.4]),np.array([0.1,0.4]))
print(0.18000000000000005 == b[0], " ", b[1] == 0.5448788923735801)


print("----TEST1.3 ------")


features, targets, classes = load_iris()
for e in features:
    if(e[0]== 6.3 and e[1] == 2.5 and e[2]==4.9 and e[3]==1.5):
        x=e
(train_features, train_targets), (test_features, test_targets) = \
    split_train_test(features, targets)
#x = train_features[0, :]
np.random.seed(1234)
K=3
M=10
D=4
W1 = 2 * np.random.rand(D + 1, M) - 1
W2 = 2 * np.random.rand(M + 1, K) - 1
y, z0, z1, a1, a2 =ffnn(x, M, K, W1, W2)
print("y=",y)
print("z0=",z0)
print("z1=",z1)
print("a0=",a1)
print("a1=",a2)




print("----TEST 1.4----")
#Remember : Non conosci la classe siccome hai saccheggiato il valore ciclatelo (target y)
np.random.seed(42)

K = 3  # number of classes
M = 6
D = train_features.shape[1]
target_y = np.zeros(K)
target_y[targets[0]] = 1.0
W1 = 2 * np.random.rand(D + 1, M) - 1
W2 = 2 * np.random.rand(M + 1, K) - 1

#y, dE1, dE2 =
backprop(x, target_y, M, K, W1, W2)

print(W1)
print(W2)


print("----- TEST 2.1 -----")

np.random.seed(1234)
(train_features, train_targets), (test_features, test_targets) = \
        split_train_test(features, targets)

K = 3  # number of classes
M = 6
D = train_features.shape[1]
# Initialize two random weight matrices
W1 = 2 * np.random.rand(D + 1, M) - 1
W2 = 2 * np.random.rand(M + 1, K) - 1
W1tr, W2tr, Etotal, misclassification_rate, last_guesses =train_nn(train_features[:20, :], train_targets[:20], M, K, W1, W2, 500, 0.1)
print(misclassification_rate)
print("Calculated ",last_guesses)
print("Correct Value ",train_targets[:20])

print("----- TEST 2.2 -----")
guesses= test_nn(test_features,M,K,W1tr,W2tr)
print(guesses)
print(test_targets)


print("----- TEST 2.3 -----")
tester()
'''



print("TEST EXTRA 2")
np.random.seed(90210)
features, targets, classes = load_iris()
(train_features, train_targets), (test_features, test_targets) = split_train_test(features, targets)

print(train_features[:10,:])
x = train_features[0, :]
print(train_features[0, :])

K = 3
M = 8
D = train_features.shape[1]
W1 = 2 * np.random.rand(D + 1, M) - 1
W2 = 2 * np.random.rand(M + 1, K) - 1

print(W1)
print(W2)
y, z0, z1, a1, a2 = ffnn(x, M, K, W1, W2)

print("1)Results mine:",y)
print("2)Results teacher: [0.70954302, 0.53441718, 0.46529406]")

print("2)Results mine:",z0)
print("2)Results teacher: [1. , 4.8, 3.1, 1.6, 0.2]")

print("3)Results mine:",z1)
print("3)Results teacher: [1.00000000e+00, 4.28036933e-04, 3.24858743e-02, 1.06131693e-01, 9.89084573e-01, 9.47322339e-01, 9.74266738e-01, 4.57365902e-02, 9.51734218e-01]")

print("4)Results mine:",a1)
print("4)Results teacher: [-7.75587295, -3.39392467, -2.13087774,4.50660275, 2.88944794, 3.63390074, -3.03804111, 2.98156294]")

print("5)Results mine:",a2)
print("5)Results teacher: [ 0.89316566, 0.13788676, -0.13904734]")


target_y = np.zeros(K)
target_y[targets[0]] = 1.0
y, dE1, dE2 = backprop(x, target_y, M, K, W1, W2)

print("1)Results mine:",y)
print("2)Results teacher: [0.70954302, 0.53441718, 0.46529406]")
teacherResult= np.asmatrix([[-1.31236522e-04, -9.00275453e-03, -3.31464988e-03,3.74790282e-03, -1.91061161e-02, -5.14849162e-03,5.97235578e-03, -1.62546059e-02],[-6.29935308e-04, -4.32132218e-02, -1.59103194e-02,1.79899335e-02, -9.17093574e-02, -2.47127598e-02,2.86673078e-02, -7.80221084e-02],[-4.06833220e-04, -2.79085390e-02, -1.02754146e-02,1.16184987e-02, -5.92289600e-02, -1.59603240e-02,1.85143029e-02, -5.03892784e-02],[-2.09978436e-04, -1.44044073e-02, -5.30343980e-03,5.99664450e-03, -3.05697858e-02, -8.23758659e-03,9.55576925e-03, -2.60073695e-02],[-2.62473045e-05, -1.80055091e-03, -6.62929975e-04,7.49580563e-04, -3.82122323e-03, -1.02969832e-03,1.19447116e-03, -3.25092119e-03]])
print("All Close dE1=",np.allclose(dE1,teacherResult))
teacherResultdE2=[[-2.90456980e-01, 5.34417178e-01, 4.65294065e-01],[-1.24326315e-04, 2.28750290e-04, 1.99163044e-04],[-9.43574892e-03, 1.73610092e-02, 1.51154845e-02],[-3.08266911e-02, 5.67186000e-02, 4.93824470e-02],[-2.87286518e-01, 5.28583786e-01, 4.60215181e-01],[-2.75156385e-01, 5.06265331e-01, 4.40783462e-01],[-2.82982574e-01, 5.20664881e-01, 4.53320531e-01],[-1.32845118e-02, 2.44424195e-02, 2.12809640e-02],[-2.76437846e-01, 5.08623114e-01, 4.42836283e-01]]
print("All Close dE2=",np.allclose(dE2,teacherResultdE2))

W1tr, W2tr, Etotal, misclassification_rate, last_guesses = train_nn(train_features[:20, :], train_targets[:20], M, K, W1, W2, 500, 0.1)
ETotalTeacher= [2.32723652, 2.22793384, 2.14841145, 2.08524528, 2.03533371,1.99599349, 1.96498535, 1.94048987, 1.92105642, 1.90554286,1.89305678, 1.88290374, 1.8745441 , 1.86755849, 1.86162078,1.85647726, 1.85193088, 1.8478293 , 1.84405593, 1.84052298,1.83716608, 1.83393963, 1.8308129 , 1.82776633, 1.82478823,1.82187195, 1.81901358, 1.81621029, 1.81345932, 1.81075746,1.80810097, 1.8054856 , 1.80290686, 1.80036012, 1.79784087,1.79534476, 1.79286769, 1.79040587, 1.78795577, 1.78551419,1.78307816, 1.78064498, 1.77821215, 1.7757774 , 1.77333859,1.77089377, 1.76844112, 1.76597891, 1.76350556, 1.76101953,1.7585194 , 1.7560038 , 1.75347141, 1.75092097, 1.74835125,1.74576105, 1.74314921, 1.74051456, 1.73785596, 1.73517225,1.73246228, 1.72972489, 1.72695889, 1.72416309, 1.72133624,1.71847709, 1.71558434, 1.71265666, 1.70969267, 1.70669097,1.70365013, 1.70056868, 1.69744515, 1.69427805, 1.69106593,1.68780736, 1.684501, 1.68114562, 1.67774012, 1.67428364,1.67077556, 1.6672156 , 1.66360386, 1.65994095, 1.65622796,1.6524666 , 1.64865921, 1.6448088 , 1.64091899, 1.63699407,1.63303882, 1.62905848, 1.62505857, 1.62104471, 1.61702247,1.61299715, 1.60897364, 1.60495626, 1.60094867, 1.5969538 ,1.59297386, 1.58901033, 1.58506411, 1.5811355 , 1.57722439,1.57333032, 1.5694526 , 1.56559038, 1.56174274, 1.55790872,1.55408741, 1.55027793, 1.54647948, 1.54269136, 1.53891293,1.53514367, 1.53138313, 1.52763093, 1.52388678, 1.52015047,1.51642183, 1.51270076, 1.5089872 , 1.50528115, 1.50158263,1.49789172, 1.4942085 , 1.49053312, 1.48686572, 1.48320646,1.47955554, 1.47591317, 1.47227955, 1.46865492, 1.46503951,1.46143357, 1.45783735, 1.4542511 , 1.45067509, 1.44710956,1.44355479, 1.44001102, 1.43647853, 1.43295756, 1.42944837,1.42595121, 1.42246633, 1.41899397, 1.41553438, 1.41208777,1.40865439, 1.40523445, 1.40182817, 1.39843576, 1.39505742,1.39169335, 1.38834374, 1.38500877, 1.38168863, 1.37838348,1.37509347, 1.37181878, 1.36855955, 1.36531592, 1.36208802,1.358876, 1.35567996, 1.35250002, 1.34933629, 1.34618888,1.34305788, 1.33994338, 1.33684547, 1.33376422, 1.3306997 ,1.32765197, 1.32462111, 1.32160716, 1.31861017, 1.31563018,1.31266722, 1.30972135, 1.30679256, 1.3038809 , 1.30098637,1.29810899, 1.29524876, 1.29240568, 1.28957975, 1.28677097,1.28397931, 1.28120476, 1.27844731, 1.27570692, 1.27298356,1.2702772 , 1.2675878 , 1.26491533, 1.26225973, 1.25962095,1.25699894, 1.25439365, 1.25180501, 1.24923296, 1.24667744,1.24413837, 1.24161569, 1.23910931, 1.23661916, 1.23414516,1.23168722, 1.22924527, 1.2268192 , 1.22440893, 1.22201437,1.21963542, 1.21727199, 1.21492397, 1.21259127, 1.21027379,1.20797142, 1.20568405, 1.20341159, 1.20115392, 1.19891093,1.19668251, 1.19446856, 1.19226896, 1.19008359, 1.18791235,1.18575512, 1.18361178, 1.18148222, 1.17936632, 1.17726397,1.17517504, 1.17309942, 1.17103699, 1.16898764, 1.16695124,1.16492767, 1.16291683, 1.16091858, 1.15893281, 1.1569594 ,1.15499823, 1.15304919, 1.15111215, 1.149187 , 1.14727362,1.14537188, 1.14348168, 1.1416029 , 1.13973541, 1.13787911,1.13603387, 1.13419958, 1.13237612, 1.13056339, 1.12876126,1.12696961, 1.12518835, 1.12341735, 1.1216565 , 1.11990569,1.11816481, 1.11643375, 1.11471239, 1.11300063, 1.11129835,1.10960546, 1.10792183, 1.10624737, 1.10458197, 1.10292552,1.10127791, 1.09963905, 1.09800882, 1.09638712, 1.09477385,1.09316891, 1.0915722 , 1.08998361, 1.08840305, 1.08683042,1.08526561, 1.08370852, 1.08215907, 1.08061716, 1.07908268,1.07755554, 1.07603566, 1.07452292, 1.07301725, 1.07151854,1.07002671, 1.06854166, 1.0670633 , 1.06559155, 1.06412631,1.0626675 , 1.06121502, 1.05976879, 1.05832872, 1.05689473,1.05546673, 1.05404464, 1.05262837, 1.05121784, 1.04981296,1.04841366, 1.04701986, 1.04563146, 1.0442484 , 1.04287059,1.04149795, 1.04013041, 1.03876789, 1.03741031, 1.0360576 ,1.03470968, 1.03336647, 1.0320279 , 1.0306939 , 1.0293644 ,1.02803932, 1.02671859, 1.02540214, 1.0240899 , 1.0227818 ,1.02147777, 1.02017775, 1.01888166, 1.01758943, 1.01630101,1.01501633, 1.01373531, 1.0124579 , 1.01118403, 1.00991364,1.00864667, 1.00738305, 1.00612272, 1.00486562, 1.00361169,1.00236087, 1.0011131 , 0.99986833, 0.99862649, 0.99738753,0.99615138, 0.99491801, 0.99368734, 0.99245933, 0.99123392,0.99001105, 0.98879068, 0.98757275, 0.9863572 , 0.985144 ,0.98393308, 0.9827244 , 0.9815179 , 0.98031354, 0.97911127,0.97791104, 0.97671281, 0.97551652, 0.97432213, 0.9731296 ,0.97193887, 0.97074992, 0.96956268, 0.96837712, 0.9671932 ,0.96601087, 0.96483009, 0.96365082, 0.96247302, 0.96129665,0.96012167, 0.95894804, 0.95777572, 0.95660467, 0.95543486,0.95426626, 0.95309881, 0.95193249, 0.95076726, 0.94960309,0.94843994, 0.94727779, 0.94611658, 0.9449563 , 0.94379692,0.94263839, 0.94148068, 0.94032378, 0.93916764, 0.93801224,0.93685755, 0.93570353, 0.93455017, 0.93339743, 0.93224529,0.93109371, 0.92994268, 0.92879216, 0.92764214, 0.92649258,0.92534347, 0.92419477, 0.92304647, 0.92189854, 0.92075096,0.9196037 , 0.91845675, 0.91731009, 0.91616369, 0.91501753,0.9138716 , 0.91272587, 0.91158032, 0.91043494, 0.90928971,0.90814461, 0.90699962, 0.90585472, 0.90470991, 0.90356516,0.90242046, 0.90127579, 0.90013114, 0.8989865 , 0.89784184,0.89669716, 0.89555244, 0.89440767, 0.89326284, 0.89211794,0.89097295, 0.88982786, 0.88868266, 0.88753735, 0.8863919 ,0.88524632, 0.88410059, 0.8829547 , 0.88180865, 0.88066242,0.87951601, 0.87836941, 0.87722261, 0.8760756 , 0.87492839,0.87378096, 0.8726333 , 0.87148542, 0.8703373 , 0.86918895,0.86804034, 0.86689149, 0.86574239, 0.86459303, 0.86344342,0.86229354, 0.86114339, 0.85999298, 0.85884229, 0.85769133,0.8565401 , 0.85538859, 0.85423681, 0.85308475, 0.85193241,0.85077979, 0.84962689, 0.84847372, 0.84732027, 0.84616654,0.84501253, 0.84385825, 0.84270369, 0.84154887, 0.84039377,0.8392384 , 0.83808277, 0.83692688, 0.83577073, 0.83461432,0.83345766, 0.83230074, 0.83114358, 0.82998618, 0.82882854,0.82767067, 0.82651257, 0.82535424, 0.82419569, 0.82303693]
W1tr_teacher=[[-0.25110607, 0.92796049, -0.91225195, -0.28777929, 0.81318948, 0.23835522, -0.22360665, 0.12187613],[-0.86742106, 1.03317936, -0.12795325, 0.8859156 , 0.84864034, 0.77358935, 0.1295815 , -0.40672377],[-0.71318284,0.05932081, 0.45588421, 0.49032422, 0.56954347, -0.06139885,1.40122813, 0.72979185],[-0.72024839, -1.07713655, -0.71681752, -0.79729479, -2.61269353, 0.09370074, -1.77038908, 0.77990277],[ 0.12129967, -1.53913854, -0.35751647, 0.97434933, -0.59098694, -0.49435129, -0.2846215 , 0.65391059]]
misclassification_rateTeacher= [0.7 , 0.7 , 0.7 , 0.7 , 0.7 , 0.7 , 0.7 , 0.4 , 0.55, 0.55, 0.55,0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55,0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55,0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55,0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55,0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55,0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55,0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.5 , 0.45, 0.45,0.45, 0.35, 0.35, 0.3 , 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 ,0.2 , 0.2 , 0.2 , 0.2 , 0.2 ]
W2tr_teacher=[[-1.46173805, -0.04343924, 0.33552804],[-0.54489514, -0.05791163, -0.93626623],[ 1.0299593 , 1.25958619, -2.05995921],[-0.24239565, -0.91381051, 0.1240486 ],[-0.21688972, 0.33045176, 0.67208629],[ 2.75398285, -2.32759378, -2.17946552],[-0.83027554, -0.28835281, -0.06244675],[ 2.654379 , -0.9461879 , -0.76684353],[-1.27516546, -0.11441948, -0.53249936]]
print("W1TR=", np.allclose(W1tr,W1tr_teacher))
print("W2TR=", np.allclose(W2tr,W2tr_teacher))
print("MisclassificationRate =",np.allclose(misclassification_rate,misclassification_rateTeacher))
print("last_guesses",(np.allclose(last_guesses,[0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 2., 0., 1., 1., 1.,0., 0., 1.])))
print("Etotal-Comparison", np.allclose(Etotal,ETotalTeacher))


print("Case 1")
np.random.seed(23)
features, targets, classes = load_iris()
(train_features, train_targets), (test_features, test_targets) = split_train_test(features, targets)
x = train_features[0, :]
K = 3
M = 6
D = train_features.shape[1]
W1 = 2 * np.random.rand(D + 1, M) - 1
W2 = 2 * np.random.rand(M + 1, K) - 1
y, z0, z1, a1, a2 = ffnn(x, M, K, W1, W2)


print("y result = ", np.allclose(y,[0.3021099 , 0.48035154, 0.35292289]))
print("z0 result = ", np.allclose(z0,[1. , 6.9, 3.1, 5.4, 2.1]))
print("z1 result = ", np.allclose(z1,[1., 0.50598577, 0.81012814, 0.93167592, 0.37820463,0.14214478, 0.54717557]))
print("a1 result = ", np.allclose(a1,[ 0.02394422, 1.45084301, 2.6127227 , -0.49717567, -1.79758923,0.18926524]))
print("a1 result = ", np.allclose(a2,[-0.83727076, -0.07863435, -0.60621589]))


target_y = np.zeros(K)
target_y[targets[0]] = 1.0

y, dE1, dE2 = backprop(x, target_y, M, K, W1, W2)
print("y result = ", np.allclose(y,[0.3021099 , 0.48035154, 0.35292289]))
dE1_teacher =[[0.00430237,-0.01753789,-0.02415926,0.30105166,-0.08800872,0.11542633],[0.02968633,-0.12101146,-0.16669889,2.07725644,-0.60726014,0.79644169],[0.01333734,-0.05436747,-0.07489371,0.93326014,-0.27282702,0.35782163],[0.02323278,-0.09470462,-0.13046,1.62567895,-0.47524707,0.62330219],[0.00903497,-0.03682958,-0.05073445,0.63220848,-0.1848183,0.2423953]]
print("dE1 result = ", np.allclose(dE1,dE1_teacher))
dE2_teacher =[[-0.6978901 , 0.48035154, 0.35292289],[-0.35312246, 0.24305104, 0.17857396],[-0.56538041, 0.3891463 ,0.28591276],[-0.65020739, 0.44753196, 0.32880975],[-0.26394526, 0.18167117, 0.13347707],[-0.09920143, 0.06827946,0.05016615],[-0.38186841, 0.26283663, 0.19311078]]
print("dE2 result = ", np.allclose(dE2,dE2_teacher))


W1tr, W2tr, Etotal, misclassification_rate, last_guesses = train_nn(train_features[:20, :], train_targets[:20], M, K, W1, W2, 500, 0.1)

W1tr_teacher= [[ 0.4113604 , -0.03795444, 0.51340102, -0.12301019, -0.50337017, -0.51770395],[-1.63577531, -0.30102308, 0.44760859, -0.16212304, -0.76077129, 0.27412644],[-0.76914322, 1.29831831, 1.50732621, -1.28479466, 0.30739053, -1.56992871],[ 2.10438589, 0.75926539, -1.25962029, 1.67000656, -0.05154486, 1.21992214],[ 1.26850911, -0.24751442, -1.27533329, 0.77080041, 0.54099358, 1.22745711]]
W2tr_teacher= [[0.28081412,-1.09243887,-0.71713827],[-1.57190232,-1.42154094,2.40252082],[-0.37652149,-1.59215728,-1.05827982],[2.95225318,0.37483314,-2.54744777],[-2.00431058,1.68047786,1.45127103],[0.93431371,0.10253565,-0.41364751],[-2.07569964,1.3497858,0.15276565]]
ETotalTeacher=[2.14469834, 2.10243644, 2.07089589, 2.0456536 , 2.02444672,2.0059886 , 1.98949344, 1.97445629, 1.9605386 , 1.94750287,1.93517287, 1.92340865, 1.91209101, 1.90111272, 1.89037482,1.87978684, 1.86927025, 1.85876409, 1.84823136, 1.83766332,1.82707849, 1.81651415, 1.8060118 , 1.79560209, 1.78529582,1.77508419, 1.76494611, 1.75485799, 1.74480169, 1.73476896,1.72476201, 1.71479123, 1.7048715 , 1.6950183 , 1.68524492,1.67556102, 1.6659725 , 1.65648216, 1.64709061, 1.63779715,1.62860035, 1.61949846, 1.61048967, 1.60157225, 1.59274459,1.58400528, 1.57535314, 1.56678726, 1.55830697, 1.54991193,1.54160208, 1.53337765, 1.52523912, 1.51718723, 1.50922292,1.50134726, 1.49356145, 1.48586674, 1.47826438, 1.47075558,1.46334143, 1.45602293, 1.44880092, 1.44167603, 1.43464875,1.42771934, 1.42088788, 1.41415427, 1.40751822, 1.40097928,1.39453686, 1.38819022, 1.3819385 , 1.37578076, 1.36971594,1.36374294, 1.35786055, 1.35206755, 1.34636267, 1.34074459,1.33521198, 1.32976349, 1.32439776, 1.31911342, 1.3139091 ,1.30878343, 1.30373505, 1.2987626 , 1.29386473, 1.28904012,1.28428744, 1.27960538, 1.27499265, 1.27044799, 1.26597013,1.26155783, 1.25720987, 1.25292505, 1.24870219, 1.24454011,1.24043768, 1.23639375, 1.23240723, 1.22847702, 1.22460205,1.22078125, 1.2170136 , 1.21329808, 1.20963368, 1.20601942,1.20245434, 1.19893748, 1.19546791, 1.19204472, 1.18866701,1.1853339 , 1.18204451, 1.178798 , 1.17559354, 1.1724303 ,1.16930747, 1.16622428, 1.16317994, 1.16017369, 1.15720479,1.15427251, 1.15137612, 1.14851493, 1.14568824, 1.14289537,1.14013566, 1.13740846, 1.13471312, 1.13204902, 1.12941555,1.1268121 , 1.12423807, 1.12169289, 1.11917599, 1.11668682,1.11422481, 1.11178945, 1.1093802 , 1.10699654, 1.10463798,1.10230401, 1.09999415, 1.09770793, 1.09544487, 1.09320452,1.09098644, 1.08879017, 1.08661529, 1.08446139, 1.08232803,1.08021482, 1.07812136, 1.07604726, 1.07399213, 1.0719556 ,1.0699373 , 1.06793688, 1.06595398, 1.06398824, 1.06203934,1.06010694, 1.05819071, 1.05629033, 1.05440548, 1.05253587,1.05068119, 1.04884114, 1.04701543, 1.04520378, 1.0434059 ,1.04162153, 1.0398504 , 1.03809224, 1.03634679, 1.03461381,1.03289304, 1.03118424, 1.02948717, 1.02780159, 1.02612729,1.02446402, 1.02281158, 1.02116974, 1.01953829, 1.01791702,1.01630573, 1.01470421, 1.01311228, 1.01152973, 1.00995638,1.00839203, 1.00683652, 1.00528965, 1.00375126, 1.00222117,1.00069921, 0.99918521, 0.99767902, 0.99618047, 0.99468941,0.99320568, 0.99172913, 0.99025961, 0.98879698, 0.9873411 ,0.98589182, 0.98444901, 0.98301253, 0.98158226, 0.98015805,0.97873979, 0.97732734, 0.9759206 , 0.97451942, 0.97312371,0.97173334, 0.9703482 , 0.96896817, 0.96759315, 0.96622303,0.96485771, 0.96349708, 0.96214104, 0.96078949, 0.95944233,0.95809947, 0.95676082, 0.95542628, 0.95409575, 0.95276917,0.95144642, 0.95012744, 0.94881214, 0.94750043, 0.94619223,0.94488747, 0.94358607, 0.94228795, 0.94099304, 0.93970127,0.93841256, 0.93712685, 0.93584406, 0.93456412, 0.93328698,0.93201257, 0.93074082, 0.92947167, 0.92820507, 0.92694094,0.92567924, 0.9244199 , 0.92316288, 0.9219081 , 0.92065553,0.91940511, 0.91815678, 0.9169105 , 0.91566621, 0.91442387,0.91318343, 0.91194485, 0.91070807, 0.90947306, 0.90823976,0.90700814, 0.90577816, 0.90454978, 0.90332294, 0.90209763,0.90087379, 0.89965139, 0.89843039, 0.89721076, 0.89599247,0.89477547, 0.89355974, 0.89234524, 0.89113194, 0.88991982,0.88870883, 0.88749895, 0.88629015, 0.8850824 , 0.88387568,0.88266996, 0.88146521, 0.8802614 , 0.87905852, 0.87785653,0.87665542, 0.87545515, 0.87425571, 0.87305708, 0.87185923,0.87066215, 0.8694658 , 0.86827018, 0.86707527, 0.86588104,0.86468748, 0.86349456, 0.86230228, 0.86111062, 0.85991955,0.85872907, 0.85753916, 0.8563498 , 0.85516098, 0.85397269,0.85278491, 0.85159762, 0.85041083, 0.84922451, 0.84803866,0.84685326, 0.8456683 , 0.84448377, 0.84329966, 0.84211596,0.84093266, 0.83974976, 0.83856724, 0.83738509, 0.83620331,0.8350219 , 0.83384083, 0.83266012, 0.83147974, 0.83029969,0.82911997, 0.82794058, 0.8267615 , 0.82558273, 0.82440427,0.82322611, 0.82204824, 0.82087068, 0.8196934 , 0.81851641,0.8173397 , 0.81616327, 0.81498713, 0.81381125, 0.81263566,0.81146033, 0.81028528, 0.80911049, 0.80793598, 0.80676173,0.80558775, 0.80441403, 0.80324058, 0.8020674 , 0.80089448,0.79972183, 0.79854944, 0.79737733, 0.79620548, 0.7950339 ,0.7938626 , 0.79269157, 0.79152081, 0.79035033, 0.78918013,0.78801021, 0.78684057, 0.78567122, 0.78450215, 0.78333338,0.7821649 , 0.78099673, 0.77982885, 0.77866128, 0.77749401,0.77632706, 0.77516042, 0.7739941 , 0.77282811, 0.77166245,0.77049712, 0.76933212, 0.76816747, 0.76700317, 0.76583921,0.76467562, 0.76351238, 0.76234952, 0.76118702, 0.76002491,0.75886317, 0.75770183, 0.75654088, 0.75538033, 0.75422019,0.75306047, 0.75190116, 0.75074227, 0.74958382, 0.74842581,0.74726824, 0.74611112, 0.74495446, 0.74379826, 0.74264253,0.74148728, 0.74033252, 0.73917824, 0.73802447, 0.7368712 ,0.73571844, 0.73456621, 0.7334145 , 0.73226332, 0.73111269,0.72996261, 0.72881308, 0.72766412, 0.72651573, 0.72536791,0.72422069, 0.72307406, 0.72192803, 0.72078261, 0.71963781,0.71849364, 0.7173501 , 0.7162072 , 0.71506495, 0.71392336,0.71278243, 0.71164218, 0.71050261, 0.70936373, 0.70822554,0.70708806, 0.7059513 , 0.70481526, 0.70367994, 0.70254537,0.70141154, 0.70027846, 0.69914615, 0.69801461, 0.69688385,0.69575388, 0.6946247 , 0.69349633, 0.69236876, 0.69124202,0.69011611, 0.68899103, 0.6878668 , 0.68674342, 0.6856209 ,0.68449925, 0.68337848, 0.6822586 , 0.68113961, 0.68002152,0.67890435, 0.67778809, 0.67667276, 0.67555836, 0.67444491,0.67333241, 0.67222087, 0.67111029, 0.67000069, 0.66889208,0.66778445, 0.66667783, 0.66557221, 0.66446761, 0.66336403,0.66226148, 0.66115998, 0.66005952, 0.65896011, 0.65786177,0.65676449, 0.6556683 , 0.65457319, 0.65347918, 0.65238626,0.65129446, 0.65020377, 0.6491142 , 0.64802577, 0.64693848,0.64585233, 0.64476733, 0.6436835 , 0.64260084, 0.64151935]
misclassification_rateTeacher=[0.7 , 0.7 , 0.7 , 0.7 , 0.65, 0.65, 0.75, 0.7 , 0.7 , 0.7 , 0.65,0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.65, 0.5 , 0.5 , 0.45, 0.4 ,0.4 , 0.4 , 0.4 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 ,0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 ,0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 ,0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 ,0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 ,0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 ,0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 ,0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 ,0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 ,0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 ,0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 ,0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 ,0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.3 , 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 ,0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.15, 0.15, 0.15,0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15,0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.1 , 0.1 , 0.1 ,0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 ,0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 ,0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 ,0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 ,0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 ,0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 ,0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 ,0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 ,0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 ,0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 ,0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.05,0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0.]
print("W1TR=", np.allclose(W1tr,W1tr_teacher))
print("W2TR=", np.allclose(W2tr,W2tr_teacher))
print("Etotal-Comparison", np.allclose(Etotal,ETotalTeacher))
print("MisclassificationRate =",np.allclose(misclassification_rate,misclassification_rateTeacher))
print("last_guesses",(np.allclose(last_guesses,[2., 2., 1., 0., 2., 1., 0., 2., 0., 1., 1., 0., 2., 0., 0., 2., 1.,1., 2., 0.])))


print("Case 3")
np.random.seed(911)
features, targets, classes = load_iris()
(train_features, train_targets), (test_features, test_targets) = split_train_test(features, targets)
x = train_features[0, :]
K = 3
M = 7
D = train_features.shape[1]
W1 = 2 * np.random.rand(D + 1, M) - 1
W2 = 2 * np.random.rand(M + 1, K) - 1
y, z0, z1, a1, a2 = ffnn(x, M, K, W1, W2)


print("y result = ", np.allclose(y,[0.41749343, 0.85666471, 0.85807563]))
print("z0 result = ", np.allclose(z0,[1. , 5.7, 2.5, 5. , 2. ]))
print("z1 result = ", np.allclose(z1,[1.00000000e+00, 1.76181506e-04, 1.99555822e-03, 7.93689653e-02, 8.73887332e-01, 4.64877865e-02, 4.60257183e-01, 2.49294744e-05]))
print("a1 result = ", np.allclose(a1,[ -8.64381961,-6.21483391,-2.45095192, 1.93577576, -3.02096261,-0.15930734, -10.59943482]))
print("a2 result = ", np.allclose(a2,[-0.33307168,1.78786006,1.79939794]))


target_y = np.zeros(K)
target_y[targets[0]] = 1.0

y, dE1, dE2 = backprop(x, target_y, M, K, W1, W2)
print("y result = ", np.allclose(y,[0.41749343, 0.85666471, 0.85807563]))
dE1_teacher =[[-1.63098070e-04,5.44630698e-04,9.12192425e-02,7.89677882e-02, -7.28629667e-02,4.73597821e-01, 7.47250078e-06], [-9.29658997e-04,3.10439498e-03,5.19949682e-01, 4.50116393e-01, -4.15318910e-01,2.69950758e+00,4.25932545e-05], [-4.07745174e-04,1.36157674e-03,2.28048106e-01, 1.97419470e-01, -1.82157417e-01,1.18399455e+00, 1.86812520e-05], [-8.15490348e-04,2.72315349e-03,4.56096213e-01,3.94838941e-01, -3.64314834e-01,2.36798910e+00, 3.73625039e-05], [-3.26196139e-04,1.08926140e-03,1.82438485e-01, 1.57935576e-01, -1.45725933e-01,9.47195642e-01, 1.49450016e-05]]
print("dE1 result = ", np.allclose(dE1,dE1_teacher))
dE2_teacher =[[-5.82506575e-01,8.56664713e-01,8.58075631e-01], [-1.02626885e-04,1.50928479e-04,1.51177057e-04], [-1.16242578e-03,1.70952431e-03,1.71233987e-03], [-4.62329441e-02,6.79925919e-02,6.81045750e-02],[-5.09045116e-01,7.48628440e-01,7.49861423e-01], [-2.70794413e-02,3.98244463e-02,3.98900368e-02], [-2.68102835e-01,3.94286087e-01,3.94935472e-01], [-1.45215827e-05,2.13562010e-05,2.13913744e-05]]
print("dE2 result = ", np.allclose(dE2,dE2_teacher))


W1tr, W2tr, Etotal, misclassification_rate, last_guesses = train_nn(train_features[:20, :], train_targets[:20], M, K, W1, W2, 500, 0.1)
W1tr_teacher= [[-0.26306569,1.16719089,0.55560102, -0.87061028, -0.40790052, 0.17513421,0.84935965], [-0.35551618,0.3370578 , -0.31560584,0.56922045,0.16742593,  -0.653588, -0.84895185], [ 1.19791031,1.77705595, -1.23918716,0.13074952,1.37791026,  -0.45795388, -0.85457606], [-0.70603391, -1.25048651,1.25137008, -0.84691664, -1.60320424, 0.86189895, -0.82313187],[-0.56115673, -1.44614854, -0.00916369, -0.17716094, -1.21105382, 1.40717432, -0.17722811]]
W2tr_teacher= [[-1.36151893, -1.02034605, -0.62030626], [ 0.32346548, -1.13522484, -0.46273817], [ 1.24427433,1.36223735, -2.16597965], [-2.06472907, -0.24272685,1.23298532], [ 1.04733931,0.30947796, -0.30946742], [ 2.61245101, -2.28396996, -2.78675876],[-1.94669378,0.06793946,1.50758224], [ 0.60550106,0.73287967,0.02635622]]

ETotalTeacher=[3.22867907,2.92673708,2.68066459,2.46547401,2.29722825, 2.1753676,2.09561669,2.04351701,2.00576799,1.97567074, 1.95009253,1.92745551,1.90689541,1.88790354,1.87016011, 1.85345067,1.83762225,1.82255995,1.80817396,1.79439211, 1.78115513,1.76841362,1.75612576,1.7442557,1.73277244, 1.72164882,1.71086086,1.70038721,1.69020867,1.68030792, 1.67066916,1.66127791,1.65212081,1.64318546,1.63446031, 1.62593451,1.61759782,1.60944053,1.60145341,1.59362759, 1.58595457,1.57842609,1.57103414,1.56377089,1.55662863, 1.54959977,1.54267674,1.53585196,1.52911783,1.52246663, 1.51589046,1.50938123,1.50293052,1.49652954,1.49016904, 1.48383915,1.47752931,1.47122805,1.46492284,1.45859985, 1.45224367,1.44583696,1.43936008,1.4327906,1.42610281, 1.41926716,1.41224978,1.40501233,1.39751227,1.38970446, 1.3815446,1.37299578,1.36403888,1.35468635,1.3449961, 1.33507815,1.32508568,1.3151875,1.30553137,1.29621572, 1.28728223,1.27872719,1.2705205,1.26262239,1.25499379, 1.2476011,1.24041734,1.2334216,1.22659785,1.21993378, 1.21341971,1.20704784,1.20081166,1.19470554,1.18872448, 1.18286391,1.17711961,1.17148764,1.1659643,1.16054613, 1.15522989,1.15001256,1.14489131,1.1398635,1.13492668, 1.13007852,1.12531687,1.12063966,1.11604495,1.11153089, 1.10709571,1.10273773,1.09845536,1.09424706,1.09011141, 1.08604703,1.08205265,1.07812705,1.0742691,1.07047772, 1.06675188,1.06309062,1.05949299,1.05595811,1.05248507, 1.04907302,1.04572109,1.04242841,1.0391941,1.03601727, 1.03289703,1.02983244,1.02682257,1.02386645,1.0209631, 1.01811151,1.01531067,1.01255954,1.00985708,1.00720222, 1.0045939,1.00203105,0.9995126,0.99703748,0.99460462, 0.99221296,0.98986144,0.98754904,0.98527471,0.98303743, 0.98083622,0.97867009,0.97653806,0.97443919,0.97237256, 0.97033725,0.96833238,0.96635707,0.96441049,0.9624918, 0.9606002,0.9587349,0.95689514,0.95508018,0.9532893, 0.95152178,0.94977696,0.94805415,0.94635272,0.94467204, 0.9430115,0.9413705,0.93974848,0.93814488,0.93655915, 0.93499076,0.93343922,0.93190401,0.93038466,0.92888071, 0.92739169,0.92591718,0.92445673,0.92300994,0.9215764, 0.92015572,0.91874751,0.91735142,0.91596707,0.91459413, 0.91323224,0.91188107,0.91054032,0.90920966,0.90788879, 0.90657741,0.90527523,0.90398198,0.90269738,0.90142116, 0.90015307,0.89889284,0.89764024,0.89639503,0.89515696, 0.8939258,0.89270134,0.89148336,0.89027164,0.88906597, 0.88786615,0.88667198,0.88548326,0.88429981,0.88312143, 0.88194794,0.88077916,0.87961492,0.87845504,0.87729935, 0.87614769,0.8749999,0.87385581,0.87271527,0.87157812, 0.87044421,0.86931339,0.86818552,0.86706044,0.86593803, 0.86481813,0.86370062,0.86258535,0.8614722,0.86036103, 0.85925171,0.85814413,0.85703814,0.85593363,0.85483048, 0.85372857,0.85262778,0.85152801,0.85042913,0.84933103, 0.84823361,0.84713677,0.84604039,0.84494438,0.84384863, 0.84275306,0.84165757,0.84056207,0.83946647,0.83837068, 0.83727463,0.83617823,0.83508142,0.83398411,0.83288626, 0.83178778,0.83068863,0.82958875,0.8284881,0.82738662, 0.82628428,0.82518104,0.82407688,0.82297177,0.8218657, 0.82075866,0.81965064,0.81854164,0.81743167,0.81632074, 0.81520888,0.81409611,0.81298246,0.81186798,0.8107527, 0.80963668,0.80851997,0.80740264,0.80628475,0.80516638, 0.80404759,0.80292848,0.80180912,0.80068959,0.79956999, 0.79845041,0.79733093,0.79621165,0.79509266,0.79397405, 0.7928559,0.79173831,0.79062136,0.78950512,0.78838969, 0.78727512,0.78616149,0.78504886,0.78393729,0.78282683, 0.78171752,0.78060941,0.77950253,0.7783969,0.77729255, 0.77618948,0.77508771,0.77398723,0.77288804,0.77179013, 0.77069349,0.76959808,0.76850389,0.76741088,0.76631902, 0.76522826,0.76413857,0.76304989,0.76196219,0.76087539, 0.75978946,0.75870434,0.75761997,0.7565363,0.75545326, 0.75437079,0.75328885,0.75220737,0.75112629,0.75004556, 0.74896512,0.74788491,0.74680489,0.74572499,0.74464517, 0.74356538,0.74248556,0.74140568,0.74032569,0.73924554, 0.7381652,0.73708462,0.73600377,0.73492261,0.73384111, 0.73275924,0.73167697,0.73059426,0.7295111,0.72842745, 0.7273433,0.72625863,0.7251734,0.72408762,0.72300125, 0.72191429,0.72082671,0.71973852,0.71864969,0.71756022, 0.7164701,0.71537931,0.71428787,0.71319575,0.71210296, 0.71100949,0.70991534,0.70882051,0.707725,0.7066288, 0.70553193,0.70443438,0.70333615,0.70223726,0.70113769, 0.70003747,0.69893659,0.69783505,0.69673288,0.69563006, 0.69452662,0.69342255,0.69231787,0.69121259,0.69010672, 0.68900026,0.68789323,0.68678563,0.68567748,0.68456879, 0.68345957,0.68234983,0.68123959,0.68012886,0.67901764, 0.67790596,0.67679383,0.67568125,0.67456825,0.67345484, 0.67234103,0.67122683,0.67011227,0.66899735,0.66788209, 0.66676651,0.66565061,0.66453442,0.66341796,0.66230123, 0.66118425,0.66006703,0.65894961,0.65783198,0.65671416, 0.65559618,0.65447805,0.65335977,0.65224138,0.65112289, 0.65000431,0.64888565,0.64776695,0.6466482,0.64552944, 0.64441066,0.64329191,0.64217318,0.64105449,0.63993587, 0.63881733,0.63769888,0.63658054,0.63546233,0.63434427, 0.63322637,0.63210865,0.63099113,0.62987381,0.62875673, 0.62763989,0.62652332,0.62540702,0.62429102,0.62317533, 0.62205997,0.62094495,0.6198303,0.61871602,0.61760214, 0.61648866,0.61537562,0.61426301,0.61315087,0.6120392, 0.61092802,0.60981735,0.6087072,0.60759759,0.60648853, 0.60538004,0.60427214,0.60316485,0.60205816,0.60095211, 0.59984671,0.59874197,0.59763791,0.59653455,0.59543189, 0.59432995,0.59322876,0.59212832,0.59102864,0.58992975, 0.58883165,0.58773437,0.58663791,0.5855423,0.58444754, 0.58335365,0.58226064,0.58116852,0.58007732,0.57898705, 0.57789771,0.57680932,0.5757219,0.57463546,0.57355001, 0.57246556,0.57138213,0.57029974,0.56921839,0.56813809]
misclassification_rateTeacher=[0.7 , 0.65, 0.6 , 0.55, 0.75, 0.75, 0.7 , 0.5 , 0.45, 0.3 , 0.05, 0.1 , 0.2 , 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.2 , 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 ,0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.1 , 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]

print("W1TR=", np.allclose(W1tr,W1tr_teacher))
print("W2TR=", np.allclose(W2tr,W2tr_teacher))
print("Etotal-Comparison=", np.allclose(Etotal,ETotalTeacher))
print("MisclassificationRate =",np.allclose(misclassification_rate,misclassification_rateTeacher))
print("last_guesses=",(np.allclose(last_guesses,[2., 0., 2., 0., 0., 1., 2., 2., 1., 0., 0., 2., 2., 0., 1., 1., 0., 1., 0., 2.])))

tester()
